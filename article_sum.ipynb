{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K56kMqHyYrvw"
      },
      "source": [
        "##### Update drivers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tPiC9y0h5RB",
        "outputId": "834ef728-8796-41d5-949e-ad174d953b19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ign:1 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Fetched 252 kB in 2s (124 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "48 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "CPU times: user 58.6 ms, sys: 23.6 ms, total: 82.2 ms\n",
            "Wall time: 3.66 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!apt update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpS6zksch6PJ"
      },
      "source": [
        "##### Install libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AM6utUttYqXY",
        "outputId": "cd136a48-82c0-457b-9898-666f504c72c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tika in /usr/local/lib/python3.7/dist-packages (1.24)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tika) (57.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tika) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (1.24.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.7/dist-packages (0.8.11)\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from python-docx) (4.9.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.7)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.21.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.13.1+cu113)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.97)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.7/dist-packages (0.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.21.6)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.2.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score) (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (4.64.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (1.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (0.0.53)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses) (4.64.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: arxiv in /usr/local/lib/python3.7/dist-packages (1.4.2)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.7/dist-packages (from arxiv) (6.0.10)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.7/dist-packages (from feedparser->arxiv) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "# pdf parsing\n",
        "!pip3 install tika\n",
        "#/pdf parsing\n",
        "\n",
        "# to .docx\n",
        "!pip3 install python-docx\n",
        "# to .docx\n",
        "\n",
        "# to pdf\n",
        "!pip3 install aspose-words\n",
        "#/to pdf\n",
        "\n",
        "# text embeddings\n",
        "!pip3 install -U sentence-transformers\n",
        "!pip3 install rouge-score\n",
        "!pip3 install sacremoses\n",
        "#/text embeddings\n",
        "\n",
        "# arxiv\n",
        "!pip3 install arxiv\n",
        "#/arxiv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S07o3LXLYw-A"
      },
      "source": [
        "##### Import libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JQHTMztrdPy",
        "outputId": "219d4623-8b9d-4539-f5e3-b8c7b8349ee8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "# general\n",
        "from scipy import spatial\n",
        "from random import randint\n",
        "import itertools\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import spacy, re\n",
        "import zipfile\n",
        "import string\n",
        "\n",
        "import time\n",
        "import os\n",
        "import io\n",
        "#/general\n",
        "\n",
        "# read pdf\n",
        "from pathlib import Path\n",
        "from tika import parser\n",
        "#/read pdf\n",
        "\n",
        "# arxiv\n",
        "import arxiv\n",
        "#/arxiv\n",
        "\n",
        "# text preprocessing\n",
        "from textblob import TextBlob\n",
        "from spacy.symbols import nsubj, nsubjpass, VERB\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "#/text preprocessing\n",
        "\n",
        "# text embeddings\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from rouge_score import rouge_scorer\n",
        "#/text embeddings\n",
        "\n",
        "# to docx\n",
        "import docx\n",
        "from docx import Document\n",
        "from docx.enum.text import WD_COLOR_INDEX\n",
        "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
        "from docx.shared import Pt\n",
        "from copy import deepcopy\n",
        "#/to docx\n",
        "\n",
        "# to pdf\n",
        "import aspose.words as aw\n",
        "#/to pdf\n",
        "\n",
        "# parallel calculations\n",
        "tqdm.pandas()\n",
        "#/parallel calculations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUo_XhXyY0oA"
      },
      "source": [
        "##### UDFs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uh_IzwH20FK_"
      },
      "outputs": [],
      "source": [
        "def tfidf_sim(x,y):\n",
        "  \n",
        "  vectorizer = TfidfVectorizer()\n",
        "  X = vectorizer.fit_transform([x,y])\n",
        "  arr = X.toarray()\n",
        "\n",
        "  return cosine_similarity(arr)[0][1]\n",
        "\n",
        "def get_rouge(x,y):\n",
        "  scorer = rouge_scorer.RougeScorer(['rouge2'], use_stemmer=True)\n",
        "  scores = scorer.score(x,y)\n",
        "\n",
        "  return round(float(str(scores).split(\"fmeasure=\")[1].split(')}')[0]),2)\n",
        "\n",
        "def write_docx(title, text):\n",
        "\n",
        "  document = Document()\n",
        "\n",
        "  style = document.styles['Normal']\n",
        "  font = style.font\n",
        "  font.name = 'Times New Roman'\n",
        "  font.size = Pt(12)\n",
        "\n",
        "  paragraph = document.add_paragraph(text)\n",
        "  paragraph.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
        "  paragraph.style = document.styles['Normal']\n",
        "\n",
        "  document.save(title)\n",
        "\n",
        "def filter_triplet(final_text):\n",
        "    \n",
        "    final_text = get_unique_text(final_text)\n",
        "    doc = spacy_nlp(final_text)\n",
        "    valid_sents = []\n",
        "\n",
        "    for s in list(doc.sents):  \n",
        "        if syntax_full(s):\n",
        "            valid_sents.append(s.text)\n",
        "    \n",
        "    final_text = ' '.join(valid_sents)\n",
        "    \n",
        "    return final_text\n",
        "\n",
        "def get_unique_text(document):\n",
        "    unique_sentences = []\n",
        "    for sentence in [sent.raw for sent in TextBlob(document).sentences]:\n",
        "        if sentence not in unique_sentences:\n",
        "            unique_sentences.append(sentence)\n",
        "    return ' '.join(unique_sentences)\n",
        "\n",
        "def syntax_full(spacy_sentence):\n",
        "    result=[]\n",
        "    for token in spacy_sentence:\n",
        "        if (token.dep == nsubj or token.dep == nsubjpass) and token.head.pos == VERB:\n",
        "            result.append(token.head)\n",
        "    if result:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Language models:"
      ],
      "metadata": {
        "id": "VJqa0gGkQj_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_nlp = spacy.load('en_core_web_sm')\n",
        "spacy_nlp.max_length = 50000000"
      ],
      "metadata": {
        "id": "mIopxz0gQjIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Parameters:"
      ],
      "metadata": {
        "id": "ygTvDxhI7aY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reading_time = 10"
      ],
      "metadata": {
        "id": "iFf7PoWj7dSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Patterns:"
      ],
      "metadata": {
        "id": "au1SRUrzhsPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matches = [\"Fig \", \"Fig.\", \"Figure \", \"fig \", \"figure \"]"
      ],
      "metadata": {
        "id": "2RSuKMJvhuiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Insert arxiv id:"
      ],
      "metadata": {
        "id": "wEovSdyr-dMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arxiv_id = input()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cra7vPYQoJUe",
        "outputId": "d4ba1a62-9319-4b37-f63a-800ff39c029d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1904.00688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Arxiv url:"
      ],
      "metadata": {
        "id": "6Y8LXrRC9IMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arxiv_url = 'https://export.arxiv.org/pdf/' + str(arxiv_id) + '.pdf'"
      ],
      "metadata": {
        "id": "B2sa07U5-gmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Put abstract and conclusions:"
      ],
      "metadata": {
        "id": "vXtAw5mrAdn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search = arxiv.Search(id_list=[arxiv_id])\n",
        "paper = next(search.results())\n",
        "abstract = paper.summary"
      ],
      "metadata": {
        "id": "vVWcnPbo-41E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Print summary:"
      ],
      "metadata": {
        "id": "fOqLzW0GfbmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "abstract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8T_YJcKNMG3",
        "outputId": "ff1f3af9-3100-40bc-ea9d-c4a05e2c3aaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Summaries are important when it comes to process huge amounts of information.\\nTheir most important benefit is saving time, which we do not have much\\nnowadays. Therefore, a summary must be short, representative and readable.\\nGenerating summaries automatically can be beneficial for humans, since it can\\nsave time and help selecting relevant documents. Automatic summarization and,\\nin particular, Automatic text summarization (ATS) is not a new research field;\\nIt was known since the 50s. Since then, researchers have been active to find\\nthe perfect summarization method. In this article, we will discuss different\\nworks in automatic summarization, especially the recent ones. We will present\\nsome problems and limits which prevent works to move forward. Most of these\\nchallenges are much more related to the nature of processed languages. These\\nchallenges are interesting for academics and developers, as a path to follow in\\nthis field.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbw-mworcAvX"
      },
      "source": [
        "##### Extract pdf content:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "pdf_file = parser.from_file(arxiv_url)\n",
        "corpus = pdf_file[\"content\"].replace('.\\n\\n', '.###').replace('?\\n\\n', '.###').replace('!\\n\\n', '.###')"
      ],
      "metadata": {
        "id": "CrfSGIii81cM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7944467a-9b81-49dd-debe-9cb558856142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-09-01 14:42:22,412 [MainThread  ] [INFO ]  Retrieving https://export.arxiv.org/pdf/1904.00688.pdf to /tmp/pdf-1904.00688.pdf.\n",
            "INFO:tika.tika:Retrieving https://export.arxiv.org/pdf/1904.00688.pdf to /tmp/pdf-1904.00688.pdf.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 145 ms, sys: 23.4 ms, total: 169 ms\n",
            "Wall time: 26.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Filter by triplets:"
      ],
      "metadata": {
        "id": "uIv8DHAXSIg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "corpus = ' '.join([filter_triplet(i.strip()) for i in tqdm(sent_tokenize(corpus))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSzn5wMzRraD",
        "outputId": "5e658685-5f84-4ac2-bf90-83a9820ce409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1324/1324 [00:11<00:00, 118.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 10.8 s, sys: 105 ms, total: 10.9 s\n",
            "Wall time: 11.3 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Leave passages without figures:"
      ],
      "metadata": {
        "id": "NBmZrD2-oqvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "core_passages = [] \n",
        "\n",
        "for j in tqdm(corpus.split('.###')):\n",
        "  \n",
        "  if any(x in j for x in matches):\n",
        "    continue\n",
        "  if len(sent_tokenize(j.replace('\\n', ' '))) >= 3:\n",
        "      core_passages.append(j.replace('\\n', ' '))\n",
        "\n",
        "df_score = pd.DataFrame(zip(core_passages), columns=['Passage'])\n",
        "df_score['Abstract'] = str(abstract)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RxW9KQntsyZ",
        "outputId": "90c62ee7-8c19-4bc7-b14a-8e882a40c504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:00<00:00, 6012.99it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Order by similarity:"
      ],
      "metadata": {
        "id": "8dlOltdBt8B5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
        "\n",
        "df_score['Passage_Embedding'] = df_score['Passage'].apply(lambda x: model.encode(x))\n",
        "df_score['Abstract_Embedding'] = df_score['Abstract'].apply(lambda x: model.encode(x))\n",
        "\n",
        "df_score['Score_Rouge'] = df_score['Passage'].apply(lambda x: get_rouge(x, abstract))\n",
        "df_score['Score_TFIDF'] = df_score['Passage'].apply(lambda x: tfidf_sim(x, abstract))\n",
        "df_score['Score_Cos'] = df_score.apply(lambda x: (1-spatial.distance.cosine(x['Abstract_Embedding'], x['Passage_Embedding'])), axis=1)\n",
        "\n",
        "df_score['Score'] = (df_score['Score_Rouge'] + df_score['Score_Cos'] + df_score['Score_TFIDF'])/3\n",
        "\n",
        "df_score = df_score.sort_values('Score', ascending=False)\n",
        "df_score = df_score[df_score['Score'] > 0]\n",
        "df_score.reset_index(inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9HOBT5AQoca",
        "outputId": "6283e458-82cd-4cc2-b54c-b99a7037a927"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.35 s, sys: 396 ms, total: 3.75 s\n",
            "Wall time: 5.93 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Extract summary:"
      ],
      "metadata": {
        "id": "nsbFpORK7FGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "core_passages = []\n",
        "\n",
        "for i in tqdm(list(df_score['Passage'])):\n",
        "  if len(word_tokenize(' '.join(core_passages) + ' ' + str(i))) < 238*reading_time:\n",
        "    core_passages.append(i)\n",
        "\n",
        "summary = '.\\n\\n'.join(core_passages) + '.'\n",
        "summary = summary.strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6E0yjcJJT3o",
        "outputId": "3151b758-62ea-4e57-d3bb-6a87de3d8af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:01<00:00, 74.46it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pQjt4v4FW0r"
      },
      "source": [
        "##### Write summaries to docx and pdf:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QslYBawDN3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06e06e5f-540b-435e-93a9-a476f784db76"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<aspose.words.saving.SaveOutputParameters object at 0x7f9754bb5950>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "write_docx('summ.docx', summary)\n",
        "doc = aw.Document(\"summ.docx\")\n",
        "doc.save(\"summ.pdf\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "K56kMqHyYrvw",
        "cpS6zksch6PJ",
        "S07o3LXLYw-A",
        "YUo_XhXyY0oA",
        "VJqa0gGkQj_7",
        "ygTvDxhI7aY5",
        "au1SRUrzhsPP",
        "wEovSdyr-dMr",
        "6Y8LXrRC9IMQ",
        "vXtAw5mrAdn0",
        "fOqLzW0GfbmR",
        "fbw-mworcAvX",
        "uIv8DHAXSIg3",
        "NBmZrD2-oqvR",
        "8dlOltdBt8B5",
        "nsbFpORK7FGz"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}